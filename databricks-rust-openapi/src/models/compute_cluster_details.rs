use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeClusterDetails {
    /// Creator user name. The field won't be included in the response if the user has already been deleted.
    #[serde(rename = "creator_user_name", skip_serializing_if = "Option::is_none")]
    pub creator_user_name: Option<String>,
    /// A message associated with the most recent state transition (Eg., the reason why the cluster entered a `TERMINATED` state).
    #[serde(rename = "state_message", skip_serializing_if = "Option::is_none")]
    pub state_message: Option<String>,
    /// Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. This feature requires specific AWS permissions to function correctly - refer to the User Guide for more details.
    #[serde(rename = "enable_elastic_disk", skip_serializing_if = "Option::is_none")]
    pub enable_elastic_disk: Option<bool>,
    /// Number of CPU cores available for this cluster. Note that this can be fractional, Eg. 7.5 cores, since certain node types are configured to share cores between Spark nodes on the same instance.
    #[serde(rename = "cluster_cores", skip_serializing_if = "Option::is_none")]
    pub cluster_cores: Option<f64>,
    #[serde(rename = "ssh_public_keys", skip_serializing_if = "Option::is_none")]
    pub ssh_public_keys: Option<Vec<String>>,
    /// Additional tags for cluster resources. Databricks will tag all cluster resources (Eg., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes:  - Currently, Databricks allows at most 45 custom tags  - Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags
    #[serde(rename = "custom_tags", default, skip_serializing_if = "Option::is_none")]
    pub custom_tags: Option<::std::collections::HashMap<String, String>>,
    /// Attributes related to clusters running on Microsoft Azure. If not specified at cluster creation, a set of default values will be used.
    #[serde(rename = "azure_attributes", skip_serializing_if = "Option::is_none")]
    pub azure_attributes: Option<Box<crate::models::ComputeAzureAttributes>>,
    /// Node on which the Spark driver resides. The driver node contains the Spark master and the <Databricks> application that manages the per-notebook Spark REPLs.
    #[serde(rename = "driver", skip_serializing_if = "Option::is_none")]
    pub driver: Option<Box<crate::models::ComputeSparkNode>>,
    /// Information about why the cluster was terminated. This field only appears when the cluster is in a `TERMINATING` or `TERMINATED` state.
    #[serde(rename = "termination_reason", skip_serializing_if = "Option::is_none")]
    pub termination_reason: Option<Box<crate::models::ComputeTerminationReason>>,
    #[serde(rename = "init_scripts", skip_serializing_if = "Option::is_none")]
    pub init_scripts: Option<Vec<crate::models::ComputeInitScriptInfo>>,
    /// The Spark version of the cluster, Eg. `3.3.x-scala2.11`. A list of available Spark versions can be retrieved by using the :method:clusters/sparkVersions API call. 
    #[serde(rename = "spark_version", skip_serializing_if = "Option::is_none")]
    pub spark_version: Option<String>,
    /// The optional ID of the instance pool for the driver of the cluster belongs. The pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not assigned.
    #[serde(rename = "driver_instance_pool_id", skip_serializing_if = "Option::is_none")]
    pub driver_instance_pool_id: Option<String>,
    #[serde(rename = "data_security_mode", skip_serializing_if = "Option::is_none")]
    pub data_security_mode: Option<crate::models::ComputeDataSecurityMode>,
    /// Tags that are added by Databricks regardless of any `custom_tags`, including:    - Vendor: Databricks    - Creator: <username_of_creator>    - ClusterName: <name_of_cluster>    - ClusterId: <id_of_cluster>    - Name: <Databricks internal use>
    #[serde(rename = "default_tags", default, skip_serializing_if = "Option::is_none")]
    pub default_tags: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "executors", skip_serializing_if = "Option::is_none")]
    pub executors: Option<Vec<crate::models::ComputeSparkNode>>,
    /// Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later.
    #[serde(rename = "autoscale", skip_serializing_if = "Option::is_none")]
    pub autoscale: Option<Box<crate::models::ComputeAutoScale>>,
    #[serde(rename = "cluster_source", skip_serializing_if = "Option::is_none")]
    pub cluster_source: Option<crate::models::ComputeClusterSource>,
    /// The node type of the Spark driver. Note that this field is optional; if unset, the driver node type will be set as the same value as `node_type_id` defined above. 
    #[serde(rename = "driver_node_type_id", skip_serializing_if = "Option::is_none")]
    pub driver_node_type_id: Option<String>,
    /// The ID of the cluster policy used to create the cluster if applicable.
    #[serde(rename = "policy_id", skip_serializing_if = "Option::is_none")]
    pub policy_id: Option<String>,
    /// Cluster log delivery status.
    #[serde(rename = "cluster_log_status", skip_serializing_if = "Option::is_none")]
    pub cluster_log_status: Option<Box<crate::models::ComputeLogSyncStatus>>,
    /// Attributes related to clusters running on Amazon Web Services. If not specified at cluster creation, a set of default values will be used.
    #[serde(rename = "aws_attributes", skip_serializing_if = "Option::is_none")]
    pub aws_attributes: Option<Box<crate::models::ComputeAwsAttributes>>,
    /// Whether to enable LUKS on cluster VMs' local disks
    #[serde(rename = "enable_local_disk_encryption", skip_serializing_if = "Option::is_none")]
    pub enable_local_disk_encryption: Option<bool>,
    /// An object containing a set of optional, user-specified Spark configuration key-value pairs. Users can also pass in a string of extra JVM options to the driver and the executors via `Sparkdriver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively. 
    #[serde(rename = "spark_conf", default, skip_serializing_if = "Option::is_none")]
    pub spark_conf: Option<::std::collections::HashMap<String, String>>,
    /// Port on which Spark JDBC server is listening, in the driver nod. No service will be listeningon on this port in executor nodes.
    #[serde(rename = "jdbc_port", skip_serializing_if = "Option::is_none")]
    pub jdbc_port: Option<i32>,
    /// Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.  Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned.
    #[serde(rename = "num_workers", skip_serializing_if = "Option::is_none")]
    pub num_workers: Option<i32>,
    #[serde(rename = "docker_image", skip_serializing_if = "Option::is_none")]
    pub docker_image: Option<Box<crate::models::ComputeDockerImage>>,
    /// Attributes related to clusters running on Google Cloud Platform. If not specified at cluster creation, a set of default values will be used.
    #[serde(rename = "gcp_attributes", skip_serializing_if = "Option::is_none")]
    pub gcp_attributes: Option<Box<crate::models::ComputeGcpAttributes>>,
    /// A canonical SparkContext identifier. This value *does* change when the Spark driver restarts. The pair `(cluster_id, spark_context_id)` is a globally unique identifier over all Spark contexts.
    #[serde(rename = "spark_context_id", skip_serializing_if = "Option::is_none")]
    pub spark_context_id: Option<i64>,
    /// Total amount of cluster memory, in megabytes
    #[serde(rename = "cluster_memory_mb", skip_serializing_if = "Option::is_none")]
    pub cluster_memory_mb: Option<i64>,
    /// Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. Users can also set this value to 0 to explicitly disable automatic termination.
    #[serde(rename = "autotermination_minutes", skip_serializing_if = "Option::is_none")]
    pub autotermination_minutes: Option<i32>,
    /// Canonical identifier for the cluster. This id is retained during cluster restarts and resizes, while each new cluster has a globally unique id.
    #[serde(rename = "cluster_id", skip_serializing_if = "Option::is_none")]
    pub cluster_id: Option<String>,
    /// Current state of the cluster.
    #[serde(rename = "state", skip_serializing_if = "Option::is_none")]
    pub state: Option<crate::models::ComputeState>,
    /// 
    #[serde(rename = "workload_type", skip_serializing_if = "Option::is_none")]
    pub workload_type: Option<Box<crate::models::ComputeWorkloadType>>,
    /// the timestamp that the cluster was started/restarted
    #[serde(rename = "last_restarted_time", skip_serializing_if = "Option::is_none")]
    pub last_restarted_time: Option<i64>,
    /// The optional ID of the instance pool to which the cluster belongs.
    #[serde(rename = "instance_pool_id", skip_serializing_if = "Option::is_none")]
    pub instance_pool_id: Option<String>,
    /// Time when the cluster driver last lost its state (due to a restart or driver failure).
    #[serde(rename = "last_state_loss_time", skip_serializing_if = "Option::is_none")]
    pub last_state_loss_time: Option<i64>,
    /// Single user name if data_security_mode is `SINGLE_USER`
    #[serde(rename = "single_user_name", skip_serializing_if = "Option::is_none")]
    pub single_user_name: Option<String>,
    /// Cluster name requested by the user. This doesn't have to be unique. If not specified at creation, the cluster name will be an empty string. 
    #[serde(rename = "cluster_name", skip_serializing_if = "Option::is_none")]
    pub cluster_name: Option<String>,
    /// An object containing a set of optional, user-specified environment variable key-value pairs. Please note that key-value pair of the form (X,Y) will be exported as is (Ie., `export X='Y'`) while launching the driver and workers.  In order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending them to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all default databricks managed environmental variables are included as well.  Example Spark environment variables: `{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or `{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`
    #[serde(rename = "spark_env_vars", default, skip_serializing_if = "Option::is_none")]
    pub spark_env_vars: Option<::std::collections::HashMap<String, String>>,
    /// Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a `PENDING` state).
    #[serde(rename = "start_time", skip_serializing_if = "Option::is_none")]
    pub start_time: Option<i64>,
    /// This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call. 
    #[serde(rename = "node_type_id", skip_serializing_if = "Option::is_none")]
    pub node_type_id: Option<String>,
    /// Time (in epoch milliseconds) when the cluster was terminated, if applicable.
    #[serde(rename = "terminated_time", skip_serializing_if = "Option::is_none")]
    pub terminated_time: Option<i64>,
    /// The configuration for delivering spark logs to a long-term storage destination. Two kinds of destinations (dbfs and s3) are supported. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every `5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while the destination of executor logs is `$destination/$clusterId/executor`.
    #[serde(rename = "cluster_log_conf", skip_serializing_if = "Option::is_none")]
    pub cluster_log_conf: Option<Box<crate::models::ComputeClusterLogConf>>,
    #[serde(rename = "runtime_engine", skip_serializing_if = "Option::is_none")]
    pub runtime_engine: Option<crate::models::ComputeRuntimeEngine>,
}

impl ComputeClusterDetails {
    pub fn new() -> ComputeClusterDetails {
        ComputeClusterDetails {
            creator_user_name: None,
            state_message: None,
            enable_elastic_disk: None,
            cluster_cores: None,
            ssh_public_keys: None,
            custom_tags: None,
            azure_attributes: None,
            driver: None,
            termination_reason: None,
            init_scripts: None,
            spark_version: None,
            driver_instance_pool_id: None,
            data_security_mode: None,
            default_tags: None,
            executors: None,
            autoscale: None,
            cluster_source: None,
            driver_node_type_id: None,
            policy_id: None,
            cluster_log_status: None,
            aws_attributes: None,
            enable_local_disk_encryption: None,
            spark_conf: None,
            jdbc_port: None,
            num_workers: None,
            docker_image: None,
            gcp_attributes: None,
            spark_context_id: None,
            cluster_memory_mb: None,
            autotermination_minutes: None,
            cluster_id: None,
            state: None,
            workload_type: None,
            last_restarted_time: None,
            instance_pool_id: None,
            last_state_loss_time: None,
            single_user_name: None,
            cluster_name: None,
            spark_env_vars: None,
            start_time: None,
            node_type_id: None,
            terminated_time: None,
            cluster_log_conf: None,
            runtime_engine: None,
        }
    }
}


