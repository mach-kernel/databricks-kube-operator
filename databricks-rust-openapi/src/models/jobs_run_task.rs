use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsRunTask {
    /// The time in milliseconds it took to execute the commands in the JAR or notebook until they  completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the  `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total  duration of a multitask job run is the value of the `run_duration` field.
    #[serde(rename = "execution_duration", skip_serializing_if = "Option::is_none")]
    pub execution_duration: Option<i64>,
    /// If dbt_task, indicates that this must execute a dbt task. It requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.
    #[serde(rename = "dbt_task", skip_serializing_if = "Option::is_none")]
    pub dbt_task: Option<Box<crate::models::JobsDbtTask>>,
    /// If condition_task, specifies a condition with an outcome that can be used to control the execution of other tasks. Does not require a cluster to execute and does not support retries or notifications.
    #[serde(rename = "condition_task", skip_serializing_if = "Option::is_none")]
    pub condition_task: Option<Box<crate::models::JobsRunConditionTask>>,
    /// If sql_task, indicates that this job must execute a SQL.
    #[serde(rename = "sql_task", skip_serializing_if = "Option::is_none")]
    pub sql_task: Option<Box<crate::models::JobsSqlTask>>,
    #[serde(rename = "libraries", skip_serializing_if = "Option::is_none")]
    pub libraries: Option<Vec<crate::models::ComputeLibrary>>,
    /// If notebook_task, indicates that this job must run a notebook. This field may not be specified in conjunction with spark_jar_task.
    #[serde(rename = "notebook_task", skip_serializing_if = "Option::is_none")]
    pub notebook_task: Option<Box<crate::models::JobsNotebookTask>>,
    /// If python_wheel_task, indicates that this job must execute a PythonWheel.
    #[serde(rename = "python_wheel_task", skip_serializing_if = "Option::is_none")]
    pub python_wheel_task: Option<Box<crate::models::JobsPythonWheelTask>>,
    /// The sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt_number of 0\\. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attemptâ€™s ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.
    #[serde(rename = "attempt_number", skip_serializing_if = "Option::is_none")]
    pub attempt_number: Option<i32>,
    /// If spark_python_task, indicates that this job must run a Python file.
    #[serde(rename = "spark_python_task", skip_serializing_if = "Option::is_none")]
    pub spark_python_task: Option<Box<crate::models::JobsSparkPythonTask>>,
    /// Parameter values including resolved references
    #[serde(rename = "resolved_values", skip_serializing_if = "Option::is_none")]
    pub resolved_values: Option<Box<crate::models::JobsResolvedValues>>,
    /// The time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.
    #[serde(rename = "setup_duration", skip_serializing_if = "Option::is_none")]
    pub setup_duration: Option<i64>,
    /// If spark_jar_task, indicates that this job must run a JAR.
    #[serde(rename = "spark_jar_task", skip_serializing_if = "Option::is_none")]
    pub spark_jar_task: Option<Box<crate::models::JobsSparkJarTask>>,
    /// A unique name for the task. This field is used to refer to this task from other tasks. This field is required and must be unique within its parent job. On Update or Reset, this field is used to reference the tasks to be updated or reset.
    #[serde(rename = "task_key", skip_serializing_if = "Option::is_none")]
    pub task_key: Option<String>,
    /// The time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.
    #[serde(rename = "cleanup_duration", skip_serializing_if = "Option::is_none")]
    pub cleanup_duration: Option<i64>,
    #[serde(rename = "depends_on", skip_serializing_if = "Option::is_none")]
    pub depends_on: Option<Vec<crate::models::JobsTaskDependency>>,
    /// An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`. See :method:jobs/create for a list of possible values.
    #[serde(rename = "run_if", skip_serializing_if = "Option::is_none")]
    pub run_if: Option<crate::models::JobsRunIf>,
    /// The time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.
    #[serde(rename = "end_time", skip_serializing_if = "Option::is_none")]
    pub end_time: Option<i64>,
    /// The ID of the task run.
    #[serde(rename = "run_id", skip_serializing_if = "Option::is_none")]
    pub run_id: Option<i64>,
    #[serde(rename = "state", skip_serializing_if = "Option::is_none")]
    pub state: Option<Box<crate::models::JobsRunState>>,
    /// If pipeline_task, indicates that this job must execute a Pipeline.
    #[serde(rename = "pipeline_task", skip_serializing_if = "Option::is_none")]
    pub pipeline_task: Option<Box<crate::models::JobsPipelineTask>>,
    /// An optional specification for a remote Git repository containing the source code used by tasks. Version-controlled source code is supported by notebook, dbt, Python script, and SQL File tasks.  If `git_source` is set, these tasks retrieve the file from the remote repository by default. However, this behavior can be overridden by setting `source` to `WORKSPACE` on the task.  Note: dbt and SQL File tasks support only version-controlled sources. If dbt or SQL File tasks are used, `git_source` must be defined on the job.
    #[serde(rename = "git_source", skip_serializing_if = "Option::is_none")]
    pub git_source: Option<Box<crate::models::JobsGitSource>>,
    /// If run_job_task, indicates that this task must execute another job.
    #[serde(rename = "run_job_task", skip_serializing_if = "Option::is_none")]
    pub run_job_task: Option<Box<crate::models::JobsRunJobTask>>,
    /// If new_cluster, a description of a new cluster that is created only for this task.
    #[serde(rename = "new_cluster", skip_serializing_if = "Option::is_none")]
    pub new_cluster: Option<Box<crate::models::ComputeClusterSpec>>,
    /// The time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.
    #[serde(rename = "start_time", skip_serializing_if = "Option::is_none")]
    pub start_time: Option<i64>,
    /// An optional description for this task.
    #[serde(rename = "description", skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters
    #[serde(rename = "spark_submit_task", skip_serializing_if = "Option::is_none")]
    pub spark_submit_task: Option<Box<crate::models::JobsSparkSubmitTask>>,
    /// The cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.
    #[serde(rename = "cluster_instance", skip_serializing_if = "Option::is_none")]
    pub cluster_instance: Option<Box<crate::models::JobsClusterInstance>>,
    /// If existing_cluster_id, the ID of an existing cluster that is used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.
    #[serde(rename = "existing_cluster_id", skip_serializing_if = "Option::is_none")]
    pub existing_cluster_id: Option<String>,
}

impl JobsRunTask {
    pub fn new() -> JobsRunTask {
        JobsRunTask {
            execution_duration: None,
            dbt_task: None,
            condition_task: None,
            sql_task: None,
            libraries: None,
            notebook_task: None,
            python_wheel_task: None,
            attempt_number: None,
            spark_python_task: None,
            resolved_values: None,
            setup_duration: None,
            spark_jar_task: None,
            task_key: None,
            cleanup_duration: None,
            depends_on: None,
            run_if: None,
            end_time: None,
            run_id: None,
            state: None,
            pipeline_task: None,
            git_source: None,
            run_job_task: None,
            new_cluster: None,
            start_time: None,
            description: None,
            spark_submit_task: None,
            cluster_instance: None,
            existing_cluster_id: None,
        }
    }
}


