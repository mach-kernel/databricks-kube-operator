use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeDiskSpec {
    /// The number of disks launched for each instance: - This feature is only enabled for supported node types. - Users can choose up to the limit of the disks supported by the node type. - For node types with no OS disk, at least one disk must be specified;   otherwise, cluster creation will fail.  If disks are attached, Databricks will configure Spark to use only the disks for scratch storage, because heterogenously sized scratch devices can lead to inefficient disk utilization. If no disks are attached, Databricks will configure Spark to use instance store disks.  Note: If disks are specified, then the Spark configuration `Sparklocal.dir` will be overridden.  Disks will be mounted at: - For AWS: `/ebs0`, `/ebs1`, and etc. - For Azure: `/remote_volume0`, `/remote_volume1`, and etc.
    #[serde(rename = "disk_count", skip_serializing_if = "Option::is_none")]
    pub disk_count: Option<i32>,
    /// 
    #[serde(rename = "disk_iops", skip_serializing_if = "Option::is_none")]
    pub disk_iops: Option<i32>,
    /// The size of each disk (in GiB) launched for each instance. Values must fall into the supported range for a particular instance type.  For AWS: - General Purpose SSD: 100 - 4096 GiB - Throughput Optimized HDD: 500 - 4096 GiB  For Azure: - Premium LRS (SSD): 1 - 1023 GiB - Standard LRS (HDD): 1- 1023 GiB
    #[serde(rename = "disk_size", skip_serializing_if = "Option::is_none")]
    pub disk_size: Option<i32>,
    /// 
    #[serde(rename = "disk_throughput", skip_serializing_if = "Option::is_none")]
    pub disk_throughput: Option<i32>,
    /// The type of disks that will be launched with this cluster.
    #[serde(rename = "disk_type", skip_serializing_if = "Option::is_none")]
    pub disk_type: Option<Box<crate::models::ComputeDiskType>>,
}

impl ComputeDiskSpec {
    pub fn new() -> ComputeDiskSpec {
        ComputeDiskSpec {
            disk_count: None,
            disk_iops: None,
            disk_size: None,
            disk_throughput: None,
            disk_type: None,
        }
    }
}


