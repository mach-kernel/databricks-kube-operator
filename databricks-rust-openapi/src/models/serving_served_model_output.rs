use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ServingServedModelOutput {
    /// An object containing a set of optional, user-specified environment variable key-value pairs used for serving this model. Note: this is an experimental feature and subject to change.  Example model environment variables that refer to Databricks secrets: `{\"OPENAI_API_KEY\": \"{{secrets/my_scope/my_key}}\", \"DATABRICKS_TOKEN\": \"{{secrets/my_scope2/my_key2}}\"}`
    #[serde(rename = "environment_vars", default, skip_serializing_if = "Option::is_none")]
    pub environment_vars: Option<::std::collections::HashMap<String, String>>,
    /// ARN of the instance profile that the served model will use to access AWS resources.
    #[serde(rename = "instance_profile_arn", skip_serializing_if = "Option::is_none")]
    pub instance_profile_arn: Option<String>,
    /// The email of the user who created the served model.
    #[serde(rename = "creator", skip_serializing_if = "Option::is_none")]
    pub creator: Option<String>,
    /// The name of the served model.
    #[serde(rename = "name", skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// The name of the model in Databricks Model Registry or the full name of the model in Unity Catalog.
    #[serde(rename = "model_name", skip_serializing_if = "Option::is_none")]
    pub model_name: Option<String>,
    /// The version of the model in Databricks Model Registry or Unity Catalog to be served.
    #[serde(rename = "model_version", skip_serializing_if = "Option::is_none")]
    pub model_version: Option<String>,
    /// Whether the compute resources for the Served Model should scale down to zero.
    #[serde(rename = "scale_to_zero_enabled", skip_serializing_if = "Option::is_none")]
    pub scale_to_zero_enabled: Option<bool>,
    /// Information corresponding to the state of the Served Model.
    #[serde(rename = "state", skip_serializing_if = "Option::is_none")]
    pub state: Option<Box<crate::models::ServingServedModelState>>,
    /// The creation timestamp of the served model in Unix time.
    #[serde(rename = "creation_timestamp", skip_serializing_if = "Option::is_none")]
    pub creation_timestamp: Option<i64>,
    /// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are \"Small\" (4 - 4 provisioned concurrency), \"Medium\" (8 - 16 provisioned concurrency), and \"Large\" (16 - 64 provisioned concurrency). If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. 
    #[serde(rename = "workload_size", skip_serializing_if = "Option::is_none")]
    pub workload_size: Option<String>,
}

impl ServingServedModelOutput {
    pub fn new() -> ServingServedModelOutput {
        ServingServedModelOutput {
            environment_vars: None,
            instance_profile_arn: None,
            creator: None,
            name: None,
            model_name: None,
            model_version: None,
            scale_to_zero_enabled: None,
            state: None,
            creation_timestamp: None,
            workload_size: None,
        }
    }
}


