use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeSparkNode {
    /// The private IP address of the host instance.
    #[serde(rename = "host_private_ip", skip_serializing_if = "Option::is_none")]
    pub host_private_ip: Option<String>,
    /// Globally unique identifier for the host instance from the cloud provider.
    #[serde(rename = "instance_id", skip_serializing_if = "Option::is_none")]
    pub instance_id: Option<String>,
    /// Attributes specific to AWS for a Spark node.
    #[serde(rename = "node_aws_attributes", skip_serializing_if = "Option::is_none")]
    pub node_aws_attributes: Option<Box<crate::models::ComputeSparkNodeAwsAttributes>>,
    /// Globally unique identifier for this node.
    #[serde(rename = "node_id", skip_serializing_if = "Option::is_none")]
    pub node_id: Option<String>,
    /// Private IP address (typically a 10.Xx.x address) of the Spark node. Note that this is different from the private IP address of the host instance.
    #[serde(rename = "private_ip", skip_serializing_if = "Option::is_none")]
    pub private_ip: Option<String>,
    /// Public DNS address of this node. This address can be used to access the Spark JDBC server on the driver node. To communicate with the JDBC server, traffic must be manually authorized by adding security group rules to the \"worker-unmanaged\" security group via the AWS console.  Actually it's the public DNS address of the host instance.
    #[serde(rename = "public_dns", skip_serializing_if = "Option::is_none")]
    pub public_dns: Option<String>,
    /// The timestamp (in millisecond) when the Spark node is launched.  The start_timestamp is set right before the container is being launched. The timestamp when the container is placed on the ResourceManager, before its launch and setup by the NodeDaemon. This timestamp is the same as the creation timestamp in the database.
    #[serde(rename = "start_timestamp", skip_serializing_if = "Option::is_none")]
    pub start_timestamp: Option<i64>,
}

impl ComputeSparkNode {
    pub fn new() -> ComputeSparkNode {
        ComputeSparkNode {
            host_private_ip: None,
            instance_id: None,
            node_aws_attributes: None,
            node_id: None,
            private_ip: None,
            public_dns: None,
            start_timestamp: None,
        }
    }
}


