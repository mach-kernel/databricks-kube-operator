use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeResizeCluster {
    /// Parameters needed in order to automatically scale clusters up and down based on load. Note: autoscaling works best with DB runtime versions 3.0 or later.
    #[serde(rename = "autoscale", skip_serializing_if = "Option::is_none")]
    pub autoscale: Option<Box<crate::models::ComputeAutoScale>>,
    /// The cluster to be resized.
    #[serde(rename = "cluster_id")]
    pub cluster_id: String,
    /// Number of worker nodes that this cluster should have. A cluster has one Spark Driver and `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.  Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual current number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in `spark_info` will gradually increase from 5 to 10 as the new nodes are provisioned.
    #[serde(rename = "num_workers", skip_serializing_if = "Option::is_none")]
    pub num_workers: Option<i32>,
}

impl ComputeResizeCluster {
    pub fn new(cluster_id: String) -> ComputeResizeCluster {
        ComputeResizeCluster {
            autoscale: None,
            cluster_id,
            num_workers: None,
        }
    }
}


