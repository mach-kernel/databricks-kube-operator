use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsRepairRun {
    /// If true, repair all failed tasks. Only one of `rerun_tasks` or `rerun_all_failed_tasks` can be used.
    #[serde(rename = "rerun_all_failed_tasks", skip_serializing_if = "Option::is_none")]
    pub rerun_all_failed_tasks: Option<bool>,
    /// The ID of the latest repair. This parameter is not required when repairing a run for the first time, but must be provided on subsequent requests to repair the same run.
    #[serde(rename = "latest_repair_id", skip_serializing_if = "Option::is_none")]
    pub latest_repair_id: Option<i64>,
    /// A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`. The map is passed to the notebook and is accessible through the [Dbutilswidgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.  If not specified upon `run-now`, the triggered run uses the jobâ€™s base parameters.  notebook_params cannot be specified in conjunction with jar_params.  Use [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.  The JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes. 
    #[serde(rename = "notebook_params", default, skip_serializing_if = "Option::is_none")]
    pub notebook_params: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "dbt_commands", skip_serializing_if = "Option::is_none")]
    pub dbt_commands: Option<Vec<String>>,
    /// A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/Datajson\"}`.
    #[serde(rename = "python_named_params", default, skip_serializing_if = "Option::is_none")]
    pub python_named_params: Option<::std::collections::HashMap<String, String>>,
    /// A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.
    #[serde(rename = "sql_params", default, skip_serializing_if = "Option::is_none")]
    pub sql_params: Option<::std::collections::HashMap<String, String>>,
    /// If true, repair all tasks that depend on the tasks in `rerun_tasks`, even if they were previously successful. Can be also used in combination with `rerun_all_failed_tasks`.
    #[serde(rename = "rerun_dependent_tasks", skip_serializing_if = "Option::is_none")]
    pub rerun_dependent_tasks: Option<bool>,
    #[serde(rename = "pipeline_params", skip_serializing_if = "Option::is_none")]
    pub pipeline_params: Option<Box<crate::models::JobsPipelineParams>>,
    #[serde(rename = "spark_submit_params", skip_serializing_if = "Option::is_none")]
    pub spark_submit_params: Option<Vec<String>>,
    #[serde(rename = "rerun_tasks", skip_serializing_if = "Option::is_none")]
    pub rerun_tasks: Option<Vec<String>>,
    /// The job run ID of the run to repair. The run must not be in progress.
    #[serde(rename = "run_id")]
    pub run_id: i64,
    #[serde(rename = "jar_params", skip_serializing_if = "Option::is_none")]
    pub jar_params: Option<Vec<String>>,
    #[serde(rename = "python_params", skip_serializing_if = "Option::is_none")]
    pub python_params: Option<Vec<String>>,
}

impl JobsRepairRun {
    pub fn new(run_id: i64) -> JobsRepairRun {
        JobsRepairRun {
            rerun_all_failed_tasks: None,
            latest_repair_id: None,
            notebook_params: None,
            dbt_commands: None,
            python_named_params: None,
            sql_params: None,
            rerun_dependent_tasks: None,
            pipeline_params: None,
            spark_submit_params: None,
            rerun_tasks: None,
            run_id,
            jar_params: None,
            python_params: None,
        }
    }
}


