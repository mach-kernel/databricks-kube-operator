use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsSubmitTask {
    /// If condition_task, specifies a condition with an outcome that can be used to control the execution of other tasks. Does not require a cluster to execute and does not support retries or notifications.
    #[serde(rename = "condition_task", skip_serializing_if = "Option::is_none")]
    pub condition_task: Option<Box<crate::models::JobsConditionTask>>,
    /// If sql_task, indicates that this job must execute a SQL.
    #[serde(rename = "sql_task", skip_serializing_if = "Option::is_none")]
    pub sql_task: Option<Box<crate::models::JobsSqlTask>>,
    #[serde(rename = "libraries", skip_serializing_if = "Option::is_none")]
    pub libraries: Option<Vec<crate::models::ComputeLibrary>>,
    /// An optional timeout applied to each run of this job task. The default behavior is to have no timeout.
    #[serde(rename = "timeout_seconds", skip_serializing_if = "Option::is_none")]
    pub timeout_seconds: Option<i32>,
    /// If notebook_task, indicates that this task must run a notebook. This field may not be specified in conjunction with spark_jar_task.
    #[serde(rename = "notebook_task", skip_serializing_if = "Option::is_none")]
    pub notebook_task: Option<Box<crate::models::JobsNotebookTask>>,
    #[serde(rename = "health", skip_serializing_if = "Option::is_none")]
    pub health: Option<Box<crate::models::JobsJobsHealthRules>>,
    /// If python_wheel_task, indicates that this job must execute a PythonWheel.
    #[serde(rename = "python_wheel_task", skip_serializing_if = "Option::is_none")]
    pub python_wheel_task: Option<Box<crate::models::JobsPythonWheelTask>>,
    /// If spark_python_task, indicates that this task must run a Python file.
    #[serde(rename = "spark_python_task", skip_serializing_if = "Option::is_none")]
    pub spark_python_task: Option<Box<crate::models::JobsSparkPythonTask>>,
    /// An optional set of email addresses notified when the task run begins or completes. The default behavior is to not send any emails.
    #[serde(rename = "email_notifications", skip_serializing_if = "Option::is_none")]
    pub email_notifications: Option<Box<crate::models::JobsJobEmailNotifications>>,
    /// If spark_jar_task, indicates that this task must run a JAR.
    #[serde(rename = "spark_jar_task", skip_serializing_if = "Option::is_none")]
    pub spark_jar_task: Option<Box<crate::models::JobsSparkJarTask>>,
    /// A unique name for the task. This field is used to refer to this task from other tasks. This field is required and must be unique within its parent job. On Update or Reset, this field is used to reference the tasks to be updated or reset.
    #[serde(rename = "task_key")]
    pub task_key: String,
    #[serde(rename = "depends_on", skip_serializing_if = "Option::is_none")]
    pub depends_on: Option<Vec<crate::models::JobsTaskDependency>>,
    /// If pipeline_task, indicates that this task must execute a Pipeline.
    #[serde(rename = "pipeline_task", skip_serializing_if = "Option::is_none")]
    pub pipeline_task: Option<Box<crate::models::JobsPipelineTask>>,
    /// Optional notification settings that are used when sending email notifications for this task run.
    #[serde(rename = "notification_settings", skip_serializing_if = "Option::is_none")]
    pub notification_settings: Option<Box<crate::models::JobsTaskNotificationSettings>>,
    /// If new_cluster, a description of a cluster that is created for each run.
    #[serde(rename = "new_cluster", skip_serializing_if = "Option::is_none")]
    pub new_cluster: Option<Box<crate::models::ComputeClusterSpec>>,
    /// If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters.
    #[serde(rename = "spark_submit_task", skip_serializing_if = "Option::is_none")]
    pub spark_submit_task: Option<Box<crate::models::JobsSparkSubmitTask>>,
    /// If existing_cluster_id, the ID of an existing cluster that is used for all runs of this task. When running tasks on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.
    #[serde(rename = "existing_cluster_id", skip_serializing_if = "Option::is_none")]
    pub existing_cluster_id: Option<String>,
}

impl JobsSubmitTask {
    pub fn new(task_key: String) -> JobsSubmitTask {
        JobsSubmitTask {
            condition_task: None,
            sql_task: None,
            libraries: None,
            timeout_seconds: None,
            notebook_task: None,
            health: None,
            python_wheel_task: None,
            spark_python_task: None,
            email_notifications: None,
            spark_jar_task: None,
            task_key,
            depends_on: None,
            pipeline_task: None,
            notification_settings: None,
            new_cluster: None,
            spark_submit_task: None,
            existing_cluster_id: None,
        }
    }
}


