use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsTask {
    /// If dbt_task, indicates that this must execute a dbt task. It requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.
    #[serde(rename = "dbt_task", skip_serializing_if = "Option::is_none")]
    pub dbt_task: Option<Box<crate::models::JobsDbtTask>>,
    /// If condition_task, specifies a condition with an outcome that can be used to control the execution of other tasks. Does not require a cluster to execute and does not support retries or notifications.
    #[serde(rename = "condition_task", skip_serializing_if = "Option::is_none")]
    pub condition_task: Option<Box<crate::models::JobsConditionTask>>,
    /// If sql_task, indicates that this job must execute a SQL task.
    #[serde(rename = "sql_task", skip_serializing_if = "Option::is_none")]
    pub sql_task: Option<Box<crate::models::JobsSqlTask>>,
    #[serde(rename = "libraries", skip_serializing_if = "Option::is_none")]
    pub libraries: Option<Vec<crate::models::ComputeLibrary>>,
    /// An optional timeout applied to each run of this job task. The default behavior is to have no timeout.
    #[serde(rename = "timeout_seconds", skip_serializing_if = "Option::is_none")]
    pub timeout_seconds: Option<i32>,
    /// An optional policy to specify whether to retry a task when it times out. The default behavior is to not retry on timeout.
    #[serde(rename = "retry_on_timeout", skip_serializing_if = "Option::is_none")]
    pub retry_on_timeout: Option<bool>,
    /// If notebook_task, indicates that this task must run a notebook. This field may not be specified in conjunction with spark_jar_task.
    #[serde(rename = "notebook_task", skip_serializing_if = "Option::is_none")]
    pub notebook_task: Option<Box<crate::models::JobsNotebookTask>>,
    #[serde(rename = "health", skip_serializing_if = "Option::is_none")]
    pub health: Option<Box<crate::models::JobsJobsHealthRules>>,
    /// An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with the `FAILED` result_state or `INTERNAL_ERROR` `life_cycle_state`. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
    #[serde(rename = "max_retries", skip_serializing_if = "Option::is_none")]
    pub max_retries: Option<i32>,
    /// If python_wheel_task, indicates that this job must execute a PythonWheel.
    #[serde(rename = "python_wheel_task", skip_serializing_if = "Option::is_none")]
    pub python_wheel_task: Option<Box<crate::models::JobsPythonWheelTask>>,
    /// If spark_python_task, indicates that this task must run a Python file.
    #[serde(rename = "spark_python_task", skip_serializing_if = "Option::is_none")]
    pub spark_python_task: Option<Box<crate::models::JobsSparkPythonTask>>,
    /// An optional set of email addresses that is notified when runs of this task begin or complete as well as when this task is deleted. The default behavior is to not send any emails.
    #[serde(rename = "email_notifications", skip_serializing_if = "Option::is_none")]
    pub email_notifications: Option<Box<crate::models::JobsTaskEmailNotifications>>,
    /// If spark_jar_task, indicates that this task must run a JAR.
    #[serde(rename = "spark_jar_task", skip_serializing_if = "Option::is_none")]
    pub spark_jar_task: Option<Box<crate::models::JobsSparkJarTask>>,
    /// A unique name for the task. This field is used to refer to this task from other tasks. This field is required and must be unique within its parent job. On Update or Reset, this field is used to reference the tasks to be updated or reset.
    #[serde(rename = "task_key")]
    pub task_key: String,
    /// If job_cluster_key, this task is executed reusing the cluster specified in `Jobsettings.job_clusters`.
    #[serde(rename = "job_cluster_key", skip_serializing_if = "Option::is_none")]
    pub job_cluster_key: Option<String>,
    #[serde(rename = "depends_on", skip_serializing_if = "Option::is_none")]
    pub depends_on: Option<Vec<crate::models::JobsTaskDependency>>,
    /// An optional value specifying the condition determining whether the task is run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.  * `ALL_SUCCESS`: All dependencies have executed and succeeded * `AT_LEAST_ONE_SUCCESS`: At least one dependency has succeeded * `NONE_FAILED`: None of the dependencies have failed and at least one was executed * `ALL_DONE`: All dependencies have been completed * `AT_LEAST_ONE_FAILED`: At least one dependency failed * `ALL_FAILED`: ALl dependencies have failed 
    #[serde(rename = "run_if", skip_serializing_if = "Option::is_none")]
    pub run_if: Option<crate::models::JobsRunIf>,
    /// The key of the compute requirement, specified in `Jobsettings.compute`, to use for execution of this task.
    #[serde(rename = "compute_key", skip_serializing_if = "Option::is_none")]
    pub compute_key: Option<String>,
    /// If pipeline_task, indicates that this task must execute a Pipeline.
    #[serde(rename = "pipeline_task", skip_serializing_if = "Option::is_none")]
    pub pipeline_task: Option<Box<crate::models::JobsPipelineTask>>,
    /// Optional notification settings that are used when sending notifications to each of the `email_notifications` for this task.
    #[serde(rename = "notification_settings", skip_serializing_if = "Option::is_none")]
    pub notification_settings: Option<Box<crate::models::JobsTaskNotificationSettings>>,
    /// If run_job_task, indicates that this task must execute another job.
    #[serde(rename = "run_job_task", skip_serializing_if = "Option::is_none")]
    pub run_job_task: Option<Box<crate::models::JobsRunJobTask>>,
    /// An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
    #[serde(rename = "min_retry_interval_millis", skip_serializing_if = "Option::is_none")]
    pub min_retry_interval_millis: Option<i32>,
    /// If new_cluster, a description of a cluster that is created for only for this task.
    #[serde(rename = "new_cluster", skip_serializing_if = "Option::is_none")]
    pub new_cluster: Option<Box<crate::models::ComputeClusterSpec>>,
    /// An optional description for this task.
    #[serde(rename = "description", skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
    /// If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters.
    #[serde(rename = "spark_submit_task", skip_serializing_if = "Option::is_none")]
    pub spark_submit_task: Option<Box<crate::models::JobsSparkSubmitTask>>,
    /// If existing_cluster_id, the ID of an existing cluster that is used for all runs of this task. When running tasks on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.
    #[serde(rename = "existing_cluster_id", skip_serializing_if = "Option::is_none")]
    pub existing_cluster_id: Option<String>,
}

impl JobsTask {
    pub fn new(task_key: String) -> JobsTask {
        JobsTask {
            dbt_task: None,
            condition_task: None,
            sql_task: None,
            libraries: None,
            timeout_seconds: None,
            retry_on_timeout: None,
            notebook_task: None,
            health: None,
            max_retries: None,
            python_wheel_task: None,
            spark_python_task: None,
            email_notifications: None,
            spark_jar_task: None,
            task_key,
            job_cluster_key: None,
            depends_on: None,
            run_if: None,
            compute_key: None,
            pipeline_task: None,
            notification_settings: None,
            run_job_task: None,
            min_retry_interval_millis: None,
            new_cluster: None,
            description: None,
            spark_submit_task: None,
            existing_cluster_id: None,
        }
    }
}


