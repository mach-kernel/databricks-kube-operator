use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsRunParameters {
    #[serde(rename = "dbt_commands", skip_serializing_if = "Option::is_none")]
    pub dbt_commands: Option<Vec<String>>,
    #[serde(rename = "jar_params", skip_serializing_if = "Option::is_none")]
    pub jar_params: Option<Vec<String>>,
    /// A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`. The map is passed to the notebook and is accessible through the [Dbutilswidgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.  If not specified upon `run-now`, the triggered run uses the jobâ€™s base parameters.  notebook_params cannot be specified in conjunction with jar_params.  Use [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.  The JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes. 
    #[serde(rename = "notebook_params", default, skip_serializing_if = "Option::is_none")]
    pub notebook_params: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "pipeline_params", skip_serializing_if = "Option::is_none")]
    pub pipeline_params: Option<Box<crate::models::JobsPipelineParams>>,
    /// A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/Datajson\"}`.
    #[serde(rename = "python_named_params", default, skip_serializing_if = "Option::is_none")]
    pub python_named_params: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "python_params", skip_serializing_if = "Option::is_none")]
    pub python_params: Option<Vec<String>>,
    #[serde(rename = "spark_submit_params", skip_serializing_if = "Option::is_none")]
    pub spark_submit_params: Option<Vec<String>>,
    /// A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.
    #[serde(rename = "sql_params", default, skip_serializing_if = "Option::is_none")]
    pub sql_params: Option<::std::collections::HashMap<String, String>>,
}

impl JobsRunParameters {
    pub fn new() -> JobsRunParameters {
        JobsRunParameters {
            dbt_commands: None,
            jar_params: None,
            notebook_params: None,
            pipeline_params: None,
            python_named_params: None,
            python_params: None,
            spark_submit_params: None,
            sql_params: None,
        }
    }
}


