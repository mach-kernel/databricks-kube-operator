use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ServingServedModelInput {
    /// An object containing a set of optional, user-specified environment variable key-value pairs used for serving this model. Note: this is an experimental feature and subject to change.  Example model environment variables that refer to Databricks secrets: `{\"OPENAI_API_KEY\": \"{{secrets/my_scope/my_key}}\", \"DATABRICKS_TOKEN\": \"{{secrets/my_scope2/my_key2}}\"}`
    #[serde(rename = "environment_vars", default, skip_serializing_if = "Option::is_none")]
    pub environment_vars: Option<::std::collections::HashMap<String, String>>,
    /// ARN of the instance profile that the served model will use to access AWS resources.
    #[serde(rename = "instance_profile_arn", skip_serializing_if = "Option::is_none")]
    pub instance_profile_arn: Option<String>,
    /// The name of the model in Databricks Model Registry to be served or if the model resides in Unity Catalog, the full name of model,  in the form of __catalog_name__.__schema_name__.__model_name__. 
    #[serde(rename = "model_name")]
    pub model_name: String,
    /// The version of the model in Databricks Model Registry or Unity Catalog to be served.
    #[serde(rename = "model_version")]
    pub model_version: String,
    /// The name of a served model. It must be unique across an endpoint. If not specified, this field will default to <model-name>-<model-version>. A served model name can consist of alphanumeric characters, dashes, and underscores. 
    #[serde(rename = "name", skip_serializing_if = "Option::is_none")]
    pub name: Option<String>,
    /// Whether the compute resources for the served model should scale down to zero.
    #[serde(rename = "scale_to_zero_enabled")]
    pub scale_to_zero_enabled: bool,
    /// The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are \"Small\" (4 - 4 provisioned concurrency), \"Medium\" (8 - 16 provisioned concurrency), and \"Large\" (16 - 64 provisioned concurrency). If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. 
    #[serde(rename = "workload_size")]
    pub workload_size: String,
}

impl ServingServedModelInput {
    pub fn new(model_name: String, model_version: String, scale_to_zero_enabled: bool, workload_size: String) -> ServingServedModelInput {
        ServingServedModelInput {
            environment_vars: None,
            instance_profile_arn: None,
            model_name,
            model_version,
            name: None,
            scale_to_zero_enabled,
            workload_size,
        }
    }
}


