use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeInstancePoolAndStats {
    /// Defines the specification of the disks that will be attached to all spark containers.
    #[serde(rename = "disk_spec", skip_serializing_if = "Option::is_none")]
    pub disk_spec: Option<Box<crate::models::ComputeDiskSpec>>,
    /// Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire additional disk space when its Spark workers are running low on disk space. In AWS, this feature requires specific AWS permissions to function correctly - refer to the User Guide for more details.
    #[serde(rename = "enable_elastic_disk", skip_serializing_if = "Option::is_none")]
    pub enable_elastic_disk: Option<bool>,
    /// Additional tags for pool resources. Databricks will tag all pool resources (Eg., AWS instances and EBS volumes) with these tags in addition to `default_tags`. Notes:  - Currently, Databricks allows at most 45 custom tags
    #[serde(rename = "custom_tags", default, skip_serializing_if = "Option::is_none")]
    pub custom_tags: Option<::std::collections::HashMap<String, String>>,
    /// Attributes related to instance pools running on Azure. If not specified at pool creation, a set of default values will be used.
    #[serde(rename = "azure_attributes", skip_serializing_if = "Option::is_none")]
    pub azure_attributes: Option<Box<crate::models::ComputeInstancePoolAzureAttributes>>,
    /// Maximum number of outstanding instances to keep in the pool, including both instances used by clusters and idle instances. Clusters that require further instance provisioning will fail during upsize requests.
    #[serde(rename = "max_capacity", skip_serializing_if = "Option::is_none")]
    pub max_capacity: Option<i32>,
    #[serde(rename = "preloaded_spark_versions", skip_serializing_if = "Option::is_none")]
    pub preloaded_spark_versions: Option<Vec<String>>,
    /// Tags that are added by Databricks regardless of any `custom_tags`, including:    - Vendor: Databricks    - InstancePoolCreator: <user_id_of_creator>    - InstancePoolName: <name_of_pool>    - InstancePoolId: <id_of_pool>
    #[serde(rename = "default_tags", default, skip_serializing_if = "Option::is_none")]
    pub default_tags: Option<::std::collections::HashMap<String, String>>,
    /// Attributes related to instance pools running on Amazon Web Services. If not specified at pool creation, a set of default values will be used.
    #[serde(rename = "aws_attributes", skip_serializing_if = "Option::is_none")]
    pub aws_attributes: Option<Box<crate::models::ComputeInstancePoolAwsAttributes>>,
    /// Status of failed pending instances in the pool.
    #[serde(rename = "status", skip_serializing_if = "Option::is_none")]
    pub status: Option<Box<crate::models::ComputeInstancePoolStatus>>,
    /// Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100 characters.
    #[serde(rename = "instance_pool_name", skip_serializing_if = "Option::is_none")]
    pub instance_pool_name: Option<String>,
    /// Attributes related to instance pools running on Google Cloud Platform. If not specified at pool creation, a set of default values will be used.
    #[serde(rename = "gcp_attributes", skip_serializing_if = "Option::is_none")]
    pub gcp_attributes: Option<Box<crate::models::ComputeInstancePoolGcpAttributes>>,
    #[serde(rename = "state", skip_serializing_if = "Option::is_none")]
    pub state: Option<crate::models::ComputeInstancePoolState>,
    /// Canonical unique identifier for the pool.
    #[serde(rename = "instance_pool_id", skip_serializing_if = "Option::is_none")]
    pub instance_pool_id: Option<String>,
    /// Minimum number of idle instances to keep in the instance pool
    #[serde(rename = "min_idle_instances", skip_serializing_if = "Option::is_none")]
    pub min_idle_instances: Option<i32>,
    #[serde(rename = "preloaded_docker_images", skip_serializing_if = "Option::is_none")]
    pub preloaded_docker_images: Option<Vec<crate::models::ComputeDockerImage>>,
    /// This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the :method:clusters/listNodeTypes API call. 
    #[serde(rename = "node_type_id", skip_serializing_if = "Option::is_none")]
    pub node_type_id: Option<String>,
    /// Usage statistics about the instance pool.
    #[serde(rename = "stats", skip_serializing_if = "Option::is_none")]
    pub stats: Option<Box<crate::models::ComputeInstancePoolStats>>,
    /// Automatically terminates the extra instances in the pool cache after they are inactive for this time in minutes if min_idle_instances requirement is already met. If not set, the extra pool instances will be automatically terminated after a default timeout. If specified, the threshold must be between 0 and 10000 minutes. Users can also set this value to 0 to instantly remove idle instances from the cache if min cache size could still hold.
    #[serde(rename = "idle_instance_autotermination_minutes", skip_serializing_if = "Option::is_none")]
    pub idle_instance_autotermination_minutes: Option<i32>,
}

impl ComputeInstancePoolAndStats {
    pub fn new() -> ComputeInstancePoolAndStats {
        ComputeInstancePoolAndStats {
            disk_spec: None,
            enable_elastic_disk: None,
            custom_tags: None,
            azure_attributes: None,
            max_capacity: None,
            preloaded_spark_versions: None,
            default_tags: None,
            aws_attributes: None,
            status: None,
            instance_pool_name: None,
            gcp_attributes: None,
            state: None,
            instance_pool_id: None,
            min_idle_instances: None,
            preloaded_docker_images: None,
            node_type_id: None,
            stats: None,
            idle_instance_autotermination_minutes: None,
        }
    }
}


