use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct JobsRunNow {
    /// A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`. The map is passed to the notebook and is accessible through the [Dbutilswidgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.  If not specified upon `run-now`, the triggered run uses the jobâ€™s base parameters.  notebook_params cannot be specified in conjunction with jar_params.  Use [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.  The JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes. 
    #[serde(rename = "notebook_params", default, skip_serializing_if = "Option::is_none")]
    pub notebook_params: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "dbt_commands", skip_serializing_if = "Option::is_none")]
    pub dbt_commands: Option<Vec<String>>,
    /// A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/Datajson\"}`.
    #[serde(rename = "python_named_params", default, skip_serializing_if = "Option::is_none")]
    pub python_named_params: Option<::std::collections::HashMap<String, String>>,
    /// An optional token to guarantee the idempotency of job run requests. If a run with the provided token already exists, the request does not create a new run but returns the ID of the existing run instead. If a run with the provided token is deleted, an error is returned.  If you specify the idempotency token, upon failure you can retry until the request succeeds. Databricks guarantees that exactly one run is launched with that idempotency token.  This token must have at most 64 characters.  For more information, see [How to ensure idempotency for jobs]( https://Kbdatabricks.com/jobs/jobs-idempotency.html).
    #[serde(rename = "idempotency_token", skip_serializing_if = "Option::is_none")]
    pub idempotency_token: Option<String>,
    /// A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.
    #[serde(rename = "sql_params", default, skip_serializing_if = "Option::is_none")]
    pub sql_params: Option<::std::collections::HashMap<String, String>>,
    #[serde(rename = "pipeline_params", skip_serializing_if = "Option::is_none")]
    pub pipeline_params: Option<Box<crate::models::JobsPipelineParams>>,
    #[serde(rename = "spark_submit_params", skip_serializing_if = "Option::is_none")]
    pub spark_submit_params: Option<Vec<String>>,
    #[serde(rename = "job_parameters", skip_serializing_if = "Option::is_none")]
    pub job_parameters: Option<Vec<::std::collections::HashMap<String, String>>>,
    /// The ID of the job to be executed
    #[serde(rename = "job_id")]
    pub job_id: i64,
    #[serde(rename = "jar_params", skip_serializing_if = "Option::is_none")]
    pub jar_params: Option<Vec<String>>,
    #[serde(rename = "python_params", skip_serializing_if = "Option::is_none")]
    pub python_params: Option<Vec<String>>,
}

impl JobsRunNow {
    pub fn new(job_id: i64) -> JobsRunNow {
        JobsRunNow {
            notebook_params: None,
            dbt_commands: None,
            python_named_params: None,
            idempotency_token: None,
            sql_params: None,
            pipeline_params: None,
            spark_submit_params: None,
            job_parameters: None,
            job_id,
            jar_params: None,
            python_params: None,
        }
    }
}


