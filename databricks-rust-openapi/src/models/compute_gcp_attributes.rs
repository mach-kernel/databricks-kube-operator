use schemars::JsonSchema;
/*
 * Databricks Accounts and Workspace REST API on ALL
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */




#[derive(JsonSchema, Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct ComputeGcpAttributes {
    #[serde(rename = "availability", skip_serializing_if = "Option::is_none")]
    pub availability: Option<crate::models::ComputeGcpAvailability>,
    /// boot disk size in GB
    #[serde(rename = "boot_disk_size", skip_serializing_if = "Option::is_none")]
    pub boot_disk_size: Option<i32>,
    /// If provided, the cluster will impersonate the google service account when accessing gcloud services (like GCS). The google service account must have previously been added to the Databricks environment by an account administrator.
    #[serde(rename = "google_service_account", skip_serializing_if = "Option::is_none")]
    pub google_service_account: Option<String>,
    /// If provided, each node (workers and driver) in the cluster will have this number of local SSDs attached. Each local SSD is 375GB in size. Refer to [GCP documentation](https://Cloudgoogle.com/compute/docs/disks/local-ssd#choose_number_local_ssds) for the supported number of local SSDs for each instance type.
    #[serde(rename = "local_ssd_count", skip_serializing_if = "Option::is_none")]
    pub local_ssd_count: Option<i32>,
}

impl ComputeGcpAttributes {
    pub fn new() -> ComputeGcpAttributes {
        ComputeGcpAttributes {
            availability: None,
            boot_disk_size: None,
            google_service_account: None,
            local_ssd_count: None,
        }
    }
}


